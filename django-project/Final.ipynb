{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting and Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-55a9353e7af1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mZeroPadding2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAveragePooling2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGlobalMaxPooling2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGlobalAveragePooling2D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "import os\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Conv2D, MaxPool2D, Flatten\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vaibhav imports\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import SeparableConv2D\n",
    "from tensorflow.keras.layers import Input \n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import h5py\n",
    "import tensorflow \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_normal_path = \"dataset/training/normal\"\n",
    "train_infected_path = \"dataset/training/infected\"\n",
    "test_normal_path = \"dataset/testing/normal\"\n",
    "test_infected_path = \"dataset/testing/infected\"\n",
    "dev_normal_path = \"dataset/evaluation/normal\"\n",
    "dev_infected_path = \"dataset/evaluation/infected\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length before adding infected X-Rays to training data: 297\n",
      "Length after adding infected X-Rays to training data: 593\n",
      "Shape of training data: (593, 224, 224, 3)\n",
      "Shape of training labels: (593,)\n"
     ]
    }
   ],
   "source": [
    "Xtrain = []\n",
    "for imagePath in os.listdir(train_normal_path):\n",
    "    img  = Image.open(train_normal_path+'/'+imagePath, 'r')\n",
    "    img = img.resize((224, 224))\n",
    "    img = img.convert('RGB')\n",
    "    Xtrain.append(np.array(img))\n",
    "\n",
    "print(\"Length before adding infected X-Rays to training data:\", len(Xtrain))\n",
    "lenBefore = len(Xtrain)\n",
    "ytrain = np.zeros((len(Xtrain)))\n",
    "\n",
    "for imagePath in os.listdir(train_infected_path):\n",
    "    img  = Image.open(train_infected_path+'/'+imagePath, 'r')\n",
    "    img = img.resize((224, 224))\n",
    "    img = img.convert('RGB')\n",
    "    Xtrain.append(np.array(img))\n",
    "    \n",
    "print(\"Length after adding infected X-Rays to training data:\" ,len(Xtrain))\n",
    "ytrain = np.append(ytrain ,np.ones((len(Xtrain)-lenBefore)), 0)\n",
    "Xtrain = np.array(Xtrain)\n",
    "print(\"Shape of training data:\", Xtrain.shape)\n",
    "print(\"Shape of training labels:\", ytrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length before adding infected X-Rays to training data: 1\n",
      "Length after adding infected X-Rays to training data: 2\n",
      "Shape of dev data: (2, 224, 224, 3)\n",
      "Shape of dev labels: (2,)\n"
     ]
    }
   ],
   "source": [
    "Xdev = []\n",
    "for imagePath in os.listdir(dev_normal_path):\n",
    "    img  = Image.open(dev_normal_path+'/'+imagePath, 'r')\n",
    "    img = img.resize((224, 224))\n",
    "    img = img.convert('RGB')\n",
    "    Xdev.append(np.array(img))\n",
    "\n",
    "print(\"Length before adding infected X-Rays to training data:\", len(Xdev))\n",
    "lenBefore = len(Xdev)\n",
    "ydev = np.zeros((len(Xdev)))\n",
    "\n",
    "for imagePath in os.listdir(dev_infected_path):\n",
    "    img  = Image.open(dev_infected_path+'/'+imagePath, 'r')\n",
    "    img = img.resize((224, 224))\n",
    "    img = img.convert('RGB')\n",
    "    Xdev.append(np.array(img))\n",
    "    \n",
    "print(\"Length after adding infected X-Rays to training data:\" ,len(Xdev))\n",
    "ydev = np.append(ydev ,np.ones((len(Xdev)-lenBefore)), 0)\n",
    "Xdev = np.array(Xdev)\n",
    "print(\"Shape of dev data:\", Xdev.shape)\n",
    "print(\"Shape of dev labels:\", ydev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length before adding infected X-Rays to training data: 69\n",
      "Length after adding infected X-Rays to training data: 94\n",
      "Shape of testing data: (94, 224, 224, 3)\n",
      "Shape of testing labels: (94,)\n"
     ]
    }
   ],
   "source": [
    "Xtest = []\n",
    "for imagePath in os.listdir(test_normal_path):\n",
    "    img  = Image.open(test_normal_path+'/'+imagePath, 'r')\n",
    "    img = img.resize((224, 224))\n",
    "    img = img.convert('RGB')\n",
    "    Xtest.append(np.array(img))\n",
    "\n",
    "print(\"Length before adding infected X-Rays to training data:\", len(Xtest))\n",
    "lenBefore = len(Xtest)\n",
    "ytest = np.zeros((len(Xtest)))\n",
    "\n",
    "for imagePath in os.listdir(test_infected_path):\n",
    "    img  = Image.open(test_infected_path+'/'+imagePath, 'r')\n",
    "    img = img.resize((224, 224))\n",
    "    img = img.convert('RGB')\n",
    "    Xtest.append(np.array(img))\n",
    "    \n",
    "print(\"Length after adding infected X-Rays to training data:\" ,len(Xtest))\n",
    "ytest = np.append(ytest ,np.ones((len(Xtest)-lenBefore)), 0)\n",
    "Xtest = np.array(Xtest)\n",
    "print(\"Shape of testing data:\", Xtest.shape)\n",
    "print(\"Shape of testing labels:\", ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing and shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data: (593, 224, 224, 3)\n",
      "Shape of training labels: (593,)\n",
      "Shape of dev data: (2, 224, 224, 3)\n",
      "Shape of dev labels: (2,)\n",
      "Shape of testing data: (94, 224, 224, 3)\n",
      "Shape of testing labels: (94,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of training data:\", Xtrain.shape)\n",
    "print(\"Shape of training labels:\", ytrain.shape)\n",
    "print(\"Shape of dev data:\", Xdev.shape)\n",
    "print(\"Shape of dev labels:\", ydev.shape)\n",
    "print(\"Shape of testing data:\", Xtest.shape)\n",
    "print(\"Shape of testing labels:\", ytest.shape)\n",
    "\n",
    "Xtrain = Xtrain/255.0\n",
    "Xdev = Xdev/255.0\n",
    "Xtest = Xtest/255.0\n",
    "\n",
    "random.seed(4)\n",
    "random.shuffle(Xtrain)\n",
    "random.shuffle(Xdev)\n",
    "random.shuffle(Xtest)\n",
    "random.shuffle(ytrain)\n",
    "random.shuffle(ydev)\n",
    "random.shuffle(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "Image.fromarray(Xtrain[24], 'RGB').show()\n",
    "print(ytrain[24])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying the given data importing mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_NAME = \"dataset\\\\\"\n",
    "imagePaths=[]\n",
    "for dirname, _, filenames in os.walk(DIR_NAME):\n",
    "    for filename in filenames:\n",
    "      imagePaths.append(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "689"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(imagePaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "data = []\n",
    "labels = []\n",
    "# loop over the image paths\n",
    "for imagePath in imagePaths:\n",
    "    # extract the class label from the filename\n",
    "    label = imagePath.split(os.path.sep)[-2]\n",
    "    # load the image, swap color channels, and resize it to be a fixed\n",
    "    # 224x224 pixels while ignoring aspect ratio\n",
    "    image = cv2.imread(imagePath)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    # update the data and labels lists, respectively\n",
    "    data.append(image)\n",
    "    labels.append(label)\n",
    "# convert the data and labels to NumPy arrays while scaling the pixel\n",
    "# intensities to the range [0, 1]\n",
    "data = np.array(data) / 255.0\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(689, 224, 224, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['infected' 'normal' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'infected' 'infected' 'infected'\n",
      " 'infected' 'infected' 'infected' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal'\n",
      " 'normal' 'normal' 'normal' 'normal' 'normal']\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "lb = LabelBinarizer()\n",
    "labelss = lb.fit_transform(labels)\n",
    "labelss = to_categorical(labelss)\n",
    "print(labels)\n",
    "print(labelss)\n",
    "# partition the data into training and testing splits using 80% of\n",
    "# the data for training and the remaining 20% for testing\n",
    "(Xtrain, Xtest, ytrain, ytest) = train_test_split(data, labels,test_size=0.20, stratify=labels, random_state=42)\n",
    "# initialize the training data augmentation object\n",
    "trainAug = ImageDataGenerator(shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=15,\n",
    "    fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('normal', array([0., 1.], dtype=float32))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[-1], labelss[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((551, 224, 224, 3), (138, 224, 224, 3), (551, 2), (138, 2))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape,Xtest.shape,ytrain.shape,ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial sample model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialSampleModel(input_shape):\n",
    "    \"\"\"\n",
    "    Implementation of the HappyModel.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "        (height, width, channels) as a tuple.  \n",
    "        Note that this does not include the 'batch' as a dimension.\n",
    "        If you have a batch like 'X_train', \n",
    "        then you can provide the input_shape using\n",
    "        X_train.shape[1:]\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    X_input = Input(input_shape)\n",
    "    \n",
    "    X = ZeroPadding2D((2,2))(X_input)\n",
    "    \n",
    "    X = Conv2D(8, (7,7), strides=(1,1), name=\"conv0\")(X)\n",
    "    X = BatchNormalization(axis = 3, name=\"bn0\")(X)\n",
    "    X = Activation(\"relu\")(X)\n",
    "    \n",
    "    X = MaxPooling2D((2,2), name=\"maxpool0\")(X)\n",
    "    \n",
    "    X = Conv2D(16, (7,7), strides=(1,1), name=\"conv1\")(X)   \n",
    "    X = BatchNormalization(axis = 3, name=\"bn1\")(X)\n",
    "    X = Activation(\"relu\")(X)\n",
    "    \n",
    "    X = MaxPooling2D((2,2), name=\"maxpool1\")(X)\n",
    "    \n",
    "    X = Conv2D(32, (7,7), strides=(1,1), name=\"conv2\")(X)   \n",
    "    X = BatchNormalization(axis = 3, name=\"bn2\")(X)\n",
    "    X = Activation(\"relu\")(X)\n",
    "    \n",
    "    X = MaxPooling2D((2,2), name=\"maxpool2\")(X)\n",
    "    \n",
    "    X = Conv2D(32, (7,7), strides=(1,1), name=\"conv3\")(X)   \n",
    "    X = BatchNormalization(axis = 3, name=\"bn3\")(X)\n",
    "    X = Activation(\"relu\")(X)\n",
    "    \n",
    "    X = MaxPooling2D((2,2), name=\"maxpool3\")(X)\n",
    "    \n",
    "    X = Conv2D(32, (7,7), strides=(1,1), name=\"conv4\")(X)   \n",
    "    X = BatchNormalization(axis = 3, name=\"bn4\")(X)\n",
    "    X = Activation(\"relu\")(X)\n",
    "    \n",
    "    X = MaxPooling2D((2,2), name=\"maxpool4\")(X)\n",
    "    \n",
    "    X = Flatten()(X)\n",
    "    X = Dense(100, activation=\"sigmoid\", name=\"fc0\")(X)\n",
    "    X = Dense(2, activation=\"softmax\", name=\"fc1\")(X)\n",
    "    \n",
    "    model = Model(inputs = X_input, outputs = X, name=\"InitialModel\")\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialModel = initialSampleModel(Xtrain.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialModel.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\varun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 551 samples, validate on 138 samples\n",
      "Epoch 1/34\n",
      "551/551 [==============================] - 35s 63ms/step - loss: 0.2985 - accuracy: 0.9165 - val_loss: 0.9239 - val_accuracy: 0.4638\n",
      "Epoch 2/34\n",
      "551/551 [==============================] - 37s 66ms/step - loss: 0.1403 - accuracy: 0.9564 - val_loss: 0.6364 - val_accuracy: 0.5217\n",
      "Epoch 3/34\n",
      "551/551 [==============================] - 37s 67ms/step - loss: 0.0727 - accuracy: 0.9855 - val_loss: 1.7297 - val_accuracy: 0.4638\n",
      "Epoch 4/34\n",
      "551/551 [==============================] - 37s 67ms/step - loss: 0.0407 - accuracy: 0.9927 - val_loss: 1.5782 - val_accuracy: 0.4638\n",
      "Epoch 5/34\n",
      "551/551 [==============================] - 37s 67ms/step - loss: 0.0390 - accuracy: 0.9873 - val_loss: 1.7793 - val_accuracy: 0.4638\n",
      "Epoch 6/34\n",
      "551/551 [==============================] - 37s 67ms/step - loss: 0.0561 - accuracy: 0.9782 - val_loss: 1.5059 - val_accuracy: 0.4710\n",
      "Epoch 7/34\n",
      "551/551 [==============================] - 38s 69ms/step - loss: 0.0799 - accuracy: 0.9764 - val_loss: 0.1814 - val_accuracy: 0.9130\n",
      "Epoch 8/34\n",
      "551/551 [==============================] - 37s 67ms/step - loss: 0.0440 - accuracy: 0.9873 - val_loss: 0.4910 - val_accuracy: 0.7971\n",
      "Epoch 9/34\n",
      "551/551 [==============================] - 37s 67ms/step - loss: 0.0159 - accuracy: 0.9946 - val_loss: 0.1921 - val_accuracy: 0.9275\n",
      "Epoch 10/34\n",
      "551/551 [==============================] - 37s 67ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.0730 - val_accuracy: 0.9710\n",
      "Epoch 11/34\n",
      "551/551 [==============================] - 38s 68ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.0810 - val_accuracy: 0.9783\n",
      "Epoch 12/34\n",
      "551/551 [==============================] - 38s 69ms/step - loss: 0.0137 - accuracy: 0.9946 - val_loss: 0.0731 - val_accuracy: 0.9855\n",
      "Epoch 13/34\n",
      "551/551 [==============================] - 37s 67ms/step - loss: 0.0239 - accuracy: 0.9909 - val_loss: 0.0286 - val_accuracy: 0.9928\n",
      "Epoch 14/34\n",
      "551/551 [==============================] - 37s 68ms/step - loss: 0.0177 - accuracy: 0.9927 - val_loss: 0.0231 - val_accuracy: 0.9928\n",
      "Epoch 15/34\n",
      "551/551 [==============================] - 37s 68ms/step - loss: 0.0257 - accuracy: 0.9909 - val_loss: 0.2733 - val_accuracy: 0.8913\n",
      "Epoch 16/34\n",
      "551/551 [==============================] - 37s 68ms/step - loss: 0.0263 - accuracy: 0.9909 - val_loss: 0.7538 - val_accuracy: 0.6739\n",
      "Epoch 17/34\n",
      "551/551 [==============================] - 37s 68ms/step - loss: 0.0407 - accuracy: 0.9909 - val_loss: 0.0743 - val_accuracy: 0.9783\n",
      "Epoch 18/34\n",
      "551/551 [==============================] - 38s 69ms/step - loss: 0.0184 - accuracy: 0.9946 - val_loss: 0.0576 - val_accuracy: 0.9855\n",
      "Epoch 19/34\n",
      "551/551 [==============================] - 37s 68ms/step - loss: 0.0287 - accuracy: 0.9946 - val_loss: 0.8562 - val_accuracy: 0.7536\n",
      "Epoch 20/34\n",
      "551/551 [==============================] - 37s 68ms/step - loss: 0.0081 - accuracy: 0.9982 - val_loss: 0.0248 - val_accuracy: 0.9928\n",
      "Epoch 21/34\n",
      "551/551 [==============================] - 37s 68ms/step - loss: 0.0131 - accuracy: 0.9982 - val_loss: 0.0103 - val_accuracy: 1.0000\n",
      "Epoch 22/34\n",
      "551/551 [==============================] - 37s 67ms/step - loss: 0.0097 - accuracy: 0.9982 - val_loss: 0.0244 - val_accuracy: 0.9855\n",
      "Epoch 23/34\n",
      "551/551 [==============================] - 37s 67ms/step - loss: 0.0057 - accuracy: 0.9982 - val_loss: 0.0150 - val_accuracy: 0.9855\n",
      "Epoch 24/34\n",
      "551/551 [==============================] - 37s 67ms/step - loss: 0.0085 - accuracy: 0.9982 - val_loss: 0.0122 - val_accuracy: 0.9928\n",
      "Epoch 25/34\n",
      "551/551 [==============================] - 37s 67ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0146 - val_accuracy: 0.9928\n",
      "Epoch 26/34\n",
      "551/551 [==============================] - 37s 67ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 1.0000\n",
      "Epoch 27/34\n",
      "551/551 [==============================] - 37s 67ms/step - loss: 0.0108 - accuracy: 0.9982 - val_loss: 0.1963 - val_accuracy: 0.9275\n",
      "Epoch 28/34\n",
      "551/551 [==============================] - 38s 69ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.0362 - val_accuracy: 0.9855\n",
      "Epoch 29/34\n",
      "551/551 [==============================] - 38s 70ms/step - loss: 0.0044 - accuracy: 0.9982 - val_loss: 0.0172 - val_accuracy: 0.9928\n",
      "Epoch 30/34\n",
      "551/551 [==============================] - 57s 104ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0123 - val_accuracy: 0.9928\n",
      "Epoch 31/34\n",
      "551/551 [==============================] - 58s 105ms/step - loss: 4.1345e-04 - accuracy: 1.0000 - val_loss: 0.0053 - val_accuracy: 1.0000\n",
      "Epoch 32/34\n",
      "551/551 [==============================] - 59s 106ms/step - loss: 6.8381e-04 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 33/34\n",
      "551/551 [==============================] - 58s 104ms/step - loss: 0.0032 - accuracy: 0.9982 - val_loss: 0.0052 - val_accuracy: 1.0000\n",
      "Epoch 34/34\n",
      "551/551 [==============================] - 57s 104ms/step - loss: 6.5203e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a211599c88>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialModel.fit(x=Xtrain, y=ytrain, validation_data=(Xtest, ytest), epochs=34, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 3s 22ms/step\n",
      "\n",
      "Loss: 0.001652309414593206\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "preds = initialModel.evaluate(x = Xtest, y = ytest)\n",
    "print()\n",
    "print(\"Loss:\", preds[0])\n",
    "print(\"Accuracy:\", preds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialModel.save(\"Initial-Model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying the given model (VGG16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\varun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "baseModel = VGG16(weights=\"imagenet\", include_top=False,\n",
    "\tinput_tensor=Input(shape=(224, 224, 3)))\n",
    "# construct the head of the model that will be placed on top of the\n",
    "# the base model\n",
    "headModel = baseModel.output\n",
    "headModel = AveragePooling2D(pool_size=(4, 4))(headModel)\n",
    "headModel = Flatten(name=\"flatten\")(headModel)\n",
    "headModel = Dense(64, activation=\"relu\")(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
    "# place the head FC model on top of the base model (this will become\n",
    "# the actual model we will train)\n",
    "model = Model(inputs=baseModel.input, outputs=headModel)\n",
    "\n",
    "for layer in baseModel.layers:\n",
    "\tlayer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_chkpt = ModelCheckpoint('best_mod.h5', save_best_only=True, monitor='accuracy')\n",
    "#early_stopping = EarlyStopping(monitor='loss', restore_best_weights=False, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "[INFO] training head...\n",
      "Train on 551 samples, validate on 138 samples\n",
      "WARNING:tensorflow:From c:\\users\\varun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/34\n",
      "551/551 [==============================] - 143s 259ms/sample - loss: 0.6293 - acc: 0.6733 - val_loss: 0.4165 - val_acc: 0.9493\n",
      "Epoch 2/34\n",
      "551/551 [==============================] - 142s 257ms/sample - loss: 0.4114 - acc: 0.8639 - val_loss: 0.2724 - val_acc: 0.9565\n",
      "Epoch 3/34\n",
      "551/551 [==============================] - 141s 255ms/sample - loss: 0.3051 - acc: 0.9038 - val_loss: 0.2006 - val_acc: 0.9638\n",
      "Epoch 4/34\n",
      "551/551 [==============================] - 142s 258ms/sample - loss: 0.2434 - acc: 0.9238 - val_loss: 0.1704 - val_acc: 0.9493\n",
      "Epoch 5/34\n",
      "551/551 [==============================] - 138s 251ms/sample - loss: 0.2082 - acc: 0.9365 - val_loss: 0.1298 - val_acc: 0.9783\n",
      "Epoch 6/34\n",
      "551/551 [==============================] - 138s 251ms/sample - loss: 0.1648 - acc: 0.9601 - val_loss: 0.1167 - val_acc: 0.9710\n",
      "Epoch 7/34\n",
      "551/551 [==============================] - 137s 249ms/sample - loss: 0.1723 - acc: 0.9492 - val_loss: 0.1005 - val_acc: 0.9710\n",
      "Epoch 8/34\n",
      "551/551 [==============================] - 139s 253ms/sample - loss: 0.1331 - acc: 0.9655 - val_loss: 0.0906 - val_acc: 0.9783\n",
      "Epoch 9/34\n",
      "551/551 [==============================] - 142s 258ms/sample - loss: 0.1208 - acc: 0.9728 - val_loss: 0.0789 - val_acc: 0.9710\n",
      "Epoch 10/34\n",
      "551/551 [==============================] - 142s 258ms/sample - loss: 0.1063 - acc: 0.9710 - val_loss: 0.0750 - val_acc: 0.9783\n",
      "Epoch 11/34\n",
      "551/551 [==============================] - 138s 250ms/sample - loss: 0.0955 - acc: 0.9710 - val_loss: 0.0689 - val_acc: 0.9783\n",
      "Epoch 12/34\n",
      "551/551 [==============================] - 140s 253ms/sample - loss: 0.0936 - acc: 0.9782 - val_loss: 0.0590 - val_acc: 0.9783\n",
      "Epoch 13/34\n",
      "551/551 [==============================] - 137s 249ms/sample - loss: 0.0890 - acc: 0.9691 - val_loss: 0.0565 - val_acc: 0.9783\n",
      "Epoch 14/34\n",
      "551/551 [==============================] - 139s 253ms/sample - loss: 0.0852 - acc: 0.9728 - val_loss: 0.0525 - val_acc: 0.9855\n",
      "Epoch 15/34\n",
      "551/551 [==============================] - 139s 252ms/sample - loss: 0.0826 - acc: 0.9710 - val_loss: 0.0559 - val_acc: 0.9855\n",
      "Epoch 16/34\n",
      "551/551 [==============================] - 140s 254ms/sample - loss: 0.0625 - acc: 0.9819 - val_loss: 0.0526 - val_acc: 0.9783\n",
      "Epoch 17/34\n",
      "551/551 [==============================] - 140s 253ms/sample - loss: 0.0636 - acc: 0.9782 - val_loss: 0.0410 - val_acc: 0.9855\n",
      "Epoch 18/34\n",
      "551/551 [==============================] - 143s 260ms/sample - loss: 0.0564 - acc: 0.9873 - val_loss: 0.0390 - val_acc: 0.9855\n",
      "Epoch 19/34\n",
      "551/551 [==============================] - 144s 261ms/sample - loss: 0.0526 - acc: 0.9855 - val_loss: 0.0365 - val_acc: 0.9855\n",
      "Epoch 20/34\n",
      "551/551 [==============================] - 140s 253ms/sample - loss: 0.0537 - acc: 0.9873 - val_loss: 0.0345 - val_acc: 0.9855\n",
      "Epoch 21/34\n",
      "551/551 [==============================] - 138s 250ms/sample - loss: 0.0466 - acc: 0.9891 - val_loss: 0.0320 - val_acc: 0.9855\n",
      "Epoch 22/34\n",
      "551/551 [==============================] - 137s 248ms/sample - loss: 0.0443 - acc: 0.9873 - val_loss: 0.0455 - val_acc: 0.9928\n",
      "Epoch 23/34\n",
      "551/551 [==============================] - 139s 252ms/sample - loss: 0.0494 - acc: 0.9891 - val_loss: 0.0290 - val_acc: 0.9928\n",
      "Epoch 24/34\n",
      "551/551 [==============================] - 138s 251ms/sample - loss: 0.0387 - acc: 0.9891 - val_loss: 0.0262 - val_acc: 0.9928\n",
      "Epoch 25/34\n",
      "551/551 [==============================] - 136s 247ms/sample - loss: 0.0410 - acc: 0.9855 - val_loss: 0.0253 - val_acc: 0.9928\n",
      "Epoch 26/34\n",
      "551/551 [==============================] - 136s 246ms/sample - loss: 0.0359 - acc: 0.9927 - val_loss: 0.0234 - val_acc: 0.9928\n",
      "Epoch 27/34\n",
      "551/551 [==============================] - 137s 249ms/sample - loss: 0.0329 - acc: 0.9946 - val_loss: 0.0232 - val_acc: 0.9928\n",
      "Epoch 28/34\n",
      "551/551 [==============================] - 136s 246ms/sample - loss: 0.0384 - acc: 0.9891 - val_loss: 0.0206 - val_acc: 0.9928\n",
      "Epoch 29/34\n",
      "551/551 [==============================] - 143s 260ms/sample - loss: 0.0312 - acc: 0.9927 - val_loss: 0.0226 - val_acc: 0.9928\n",
      "Epoch 30/34\n",
      "551/551 [==============================] - 154s 279ms/sample - loss: 0.0365 - acc: 0.9873 - val_loss: 0.0176 - val_acc: 1.0000\n",
      "Epoch 31/34\n",
      "551/551 [==============================] - 141s 257ms/sample - loss: 0.0421 - acc: 0.9891 - val_loss: 0.0276 - val_acc: 0.9928\n",
      "Epoch 32/34\n",
      "551/551 [==============================] - 274s 497ms/sample - loss: 0.0264 - acc: 0.9909 - val_loss: 0.0157 - val_acc: 1.0000\n",
      "Epoch 33/34\n",
      "551/551 [==============================] - 393s 714ms/sample - loss: 0.0251 - acc: 0.9927 - val_loss: 0.0149 - val_acc: 1.0000\n",
      "Epoch 34/34\n",
      "551/551 [==============================] - 353s 641ms/sample - loss: 0.0217 - acc: 0.9964 - val_loss: 0.0163 - val_acc: 0.9928\n"
     ]
    }
   ],
   "source": [
    "INIT_LR = 1e-3\n",
    "EPOCHS = 34\n",
    "BS = 16\n",
    "# compile our model\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
    "\tmetrics=[\"accuracy\"])\n",
    "# train the head of the network\n",
    "print(\"[INFO] training head...\")\n",
    "H = model.fit(\n",
    "\tx=Xtrain, y=ytrain, batch_size=16,\n",
    "\tvalidation_data=(Xtest, ytest),\n",
    "\tepochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vaibhav model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vaibhav_model():\n",
    "    input_img = Input(shape=(224,224,3), name='ImageInput')\n",
    "    x = Conv2D(64, (3,3), activation='relu', padding='same', name='Conv1_1')(input_img)\n",
    "    x = Conv2D(64, (3,3), activation='relu', padding='same', name='Conv1_2')(x)\n",
    "    x = MaxPooling2D((2,2), name='pool1')(x)\n",
    "    \n",
    "    x = SeparableConv2D(128, (3,3), activation='relu', padding='same', name='Conv2_1')(x)\n",
    "    x = SeparableConv2D(128, (3,3), activation='relu', padding='same', name='Conv2_2')(x)\n",
    "    x = MaxPooling2D((2,2), name='pool2')(x)\n",
    "    \n",
    "    x = SeparableConv2D(256, (3,3), activation='relu', padding='same', name='Conv3_1')(x)\n",
    "    x = BatchNormalization(name='bn1')(x)\n",
    "    x = SeparableConv2D(256, (3,3), activation='relu', padding='same', name='Conv3_2')(x)\n",
    "    x = BatchNormalization(name='bn2')(x)\n",
    "    x = SeparableConv2D(256, (3,3), activation='relu', padding='same', name='Conv3_3')(x)\n",
    "    x = MaxPooling2D((2,2), name='pool3')(x)\n",
    "    x = Flatten(name='flatten')(x)\n",
    "    x = Dense(512, activation='relu', name='fc1')(x)\n",
    "    x = Dropout(0.5, name='dropout1')(x)\n",
    "    x = Dense(128, activation='relu', name='fc2')(x)\n",
    "    x = Dropout(0.5, name='dropout2')(x)\n",
    "    x = Dense(2, activation='softmax', name='fc3')(x)\n",
    "    \n",
    "    model = Model(inputs=input_img, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\varun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "ImageInput (InputLayer)      [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "Conv1_1 (Conv2D)             (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "Conv1_2 (Conv2D)             (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling2D)         (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "Conv2_1 (SeparableConv2D)    (None, 112, 112, 128)     8896      \n",
      "_________________________________________________________________\n",
      "Conv2_2 (SeparableConv2D)    (None, 112, 112, 128)     17664     \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling2D)         (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "Conv3_1 (SeparableConv2D)    (None, 56, 56, 256)       34176     \n",
      "_________________________________________________________________\n",
      "bn1 (BatchNormalization)     (None, 56, 56, 256)       1024      \n",
      "_________________________________________________________________\n",
      "Conv3_2 (SeparableConv2D)    (None, 56, 56, 256)       68096     \n",
      "_________________________________________________________________\n",
      "bn2 (BatchNormalization)     (None, 56, 56, 256)       1024      \n",
      "_________________________________________________________________\n",
      "Conv3_3 (SeparableConv2D)    (None, 56, 56, 256)       68096     \n",
      "_________________________________________________________________\n",
      "pool3 (MaxPooling2D)         (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 200704)            0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 512)               102760960 \n",
      "_________________________________________________________________\n",
      "dropout1 (Dropout)           (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout2 (Dropout)           (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "fc3 (Dense)                  (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 103,064,578\n",
      "Trainable params: 103,063,554\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "Train on 551 samples, validate on 138 samples\n",
      "WARNING:tensorflow:From c:\\users\\varun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/34\n",
      "551/551 [==============================] - 269s 487ms/sample - loss: 0.6636 - acc: 0.6243 - val_loss: 0.6982 - val_acc: 0.4638\n",
      "Epoch 2/34\n",
      "551/551 [==============================] - 291s 528ms/sample - loss: 0.2470 - acc: 0.9220 - val_loss: 1.1079 - val_acc: 0.4638\n",
      "Epoch 3/34\n",
      "551/551 [==============================] - 277s 503ms/sample - loss: 0.1322 - acc: 0.9492 - val_loss: 1.2144 - val_acc: 0.4638\n",
      "Epoch 4/34\n",
      "551/551 [==============================] - 278s 504ms/sample - loss: 0.0990 - acc: 0.9691 - val_loss: 1.2834 - val_acc: 0.4638\n",
      "Epoch 5/34\n",
      "551/551 [==============================] - 289s 524ms/sample - loss: 0.0960 - acc: 0.9691 - val_loss: 1.1945 - val_acc: 0.4638\n",
      "Epoch 6/34\n",
      "551/551 [==============================] - 283s 513ms/sample - loss: 0.0861 - acc: 0.9710 - val_loss: 1.0911 - val_acc: 0.4638\n",
      "Epoch 7/34\n",
      "551/551 [==============================] - 293s 532ms/sample - loss: 0.0620 - acc: 0.9782 - val_loss: 1.4313 - val_acc: 0.4638\n",
      "Epoch 8/34\n",
      "551/551 [==============================] - 294s 534ms/sample - loss: 0.0655 - acc: 0.9746 - val_loss: 1.4371 - val_acc: 0.4638\n",
      "Epoch 9/34\n",
      "551/551 [==============================] - 282s 512ms/sample - loss: 0.0677 - acc: 0.9728 - val_loss: 1.3077 - val_acc: 0.4638\n",
      "Epoch 10/34\n",
      "551/551 [==============================] - 284s 516ms/sample - loss: 0.0607 - acc: 0.9800 - val_loss: 0.9403 - val_acc: 0.4638\n",
      "Epoch 11/34\n",
      "551/551 [==============================] - 238s 432ms/sample - loss: 0.0428 - acc: 0.9819 - val_loss: 0.8430 - val_acc: 0.4638\n",
      "Epoch 12/34\n",
      "551/551 [==============================] - 273s 495ms/sample - loss: 0.0367 - acc: 0.9855 - val_loss: 0.6129 - val_acc: 0.5072\n",
      "Epoch 13/34\n",
      "551/551 [==============================] - 288s 523ms/sample - loss: 0.0250 - acc: 0.9891 - val_loss: 0.3216 - val_acc: 0.8623\n",
      "Epoch 14/34\n",
      "551/551 [==============================] - 267s 485ms/sample - loss: 0.0412 - acc: 0.9837 - val_loss: 0.1856 - val_acc: 0.9783\n",
      "Epoch 15/34\n",
      "551/551 [==============================] - 239s 434ms/sample - loss: 0.0258 - acc: 0.9837 - val_loss: 0.0845 - val_acc: 0.9855\n",
      "Epoch 16/34\n",
      "551/551 [==============================] - 255s 462ms/sample - loss: 0.0332 - acc: 0.9819 - val_loss: 0.2501 - val_acc: 0.8841\n",
      "Epoch 17/34\n",
      "551/551 [==============================] - 247s 449ms/sample - loss: 0.0278 - acc: 0.9927 - val_loss: 0.0461 - val_acc: 0.9855\n",
      "Epoch 18/34\n",
      "551/551 [==============================] - 234s 424ms/sample - loss: 0.0215 - acc: 0.9909 - val_loss: 0.0522 - val_acc: 0.9783\n",
      "Epoch 19/34\n",
      "551/551 [==============================] - 235s 427ms/sample - loss: 0.0223 - acc: 0.9927 - val_loss: 0.0283 - val_acc: 0.9928\n",
      "Epoch 20/34\n",
      "551/551 [==============================] - 238s 432ms/sample - loss: 0.0255 - acc: 0.9927 - val_loss: 0.0162 - val_acc: 0.9855\n",
      "Epoch 21/34\n",
      "551/551 [==============================] - 251s 455ms/sample - loss: 0.0204 - acc: 0.9927 - val_loss: 0.0454 - val_acc: 0.9855\n",
      "Epoch 22/34\n",
      "551/551 [==============================] - 238s 433ms/sample - loss: 0.0149 - acc: 0.9964 - val_loss: 0.0369 - val_acc: 0.9855\n",
      "Epoch 23/34\n",
      "551/551 [==============================] - 238s 432ms/sample - loss: 0.0164 - acc: 0.9946 - val_loss: 0.0092 - val_acc: 0.9928\n",
      "Epoch 24/34\n",
      "551/551 [==============================] - 246s 446ms/sample - loss: 0.0064 - acc: 0.9982 - val_loss: 0.0349 - val_acc: 0.9783\n",
      "Epoch 25/34\n",
      "551/551 [==============================] - 205s 372ms/sample - loss: 0.0062 - acc: 0.9964 - val_loss: 0.0098 - val_acc: 0.9928\n",
      "Epoch 26/34\n",
      "551/551 [==============================] - 207s 376ms/sample - loss: 0.0079 - acc: 0.9964 - val_loss: 0.0566 - val_acc: 0.9855\n",
      "Epoch 27/34\n",
      "551/551 [==============================] - 203s 368ms/sample - loss: 0.0082 - acc: 0.9946 - val_loss: 0.0616 - val_acc: 0.9855\n",
      "Epoch 28/34\n",
      "551/551 [==============================] - 234s 425ms/sample - loss: 0.0527 - acc: 0.9800 - val_loss: 0.5726 - val_acc: 0.8768\n",
      "Epoch 29/34\n",
      "551/551 [==============================] - 242s 439ms/sample - loss: 0.0168 - acc: 0.9946 - val_loss: 0.0082 - val_acc: 1.0000\n",
      "Epoch 30/34\n",
      "551/551 [==============================] - 240s 435ms/sample - loss: 0.0081 - acc: 0.9964 - val_loss: 0.0078 - val_acc: 1.0000\n",
      "Epoch 31/34\n",
      "551/551 [==============================] - 237s 430ms/sample - loss: 0.0039 - acc: 0.9982 - val_loss: 0.0267 - val_acc: 0.9855\n",
      "Epoch 32/34\n",
      "551/551 [==============================] - 241s 438ms/sample - loss: 0.0035 - acc: 1.0000 - val_loss: 0.0137 - val_acc: 0.9855\n",
      "Epoch 33/34\n",
      "551/551 [==============================] - 239s 434ms/sample - loss: 0.0027 - acc: 0.9982 - val_loss: 0.0196 - val_acc: 0.9928\n",
      "Epoch 34/34\n",
      "551/551 [==============================] - 224s 407ms/sample - loss: 0.0106 - acc: 0.9946 - val_loss: 0.0353 - val_acc: 0.9855\n"
     ]
    }
   ],
   "source": [
    "INIT_LR = 1e-3\n",
    "EPOCHS = 34\n",
    "BS = 16\n",
    "model =  vaibhav_model()\n",
    "model.summary()\n",
    "opt = Adam(lr=0.0001, decay=1e-5)\n",
    "#es = callbacks.EarlyStopping(patience=18)\n",
    "#chkpt = callbacks.ModelCheckpoint(filepath='best_model_todate', save_best_only=True, save_weights_only=False)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'],optimizer=opt)\n",
    "history =model.fit(x = Xtrain, y = ytrain, batch_size=16, epochs=EPOCHS, validation_data=(Xtest, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"vaibhavModel.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing external test data and testing the three models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_train_data_path_infected = \"data_upload_v2\\\\train\\\\covid\"\n",
    "ext_train_data_path_normal = \"data_upload_v2\\\\train\\\\non\"\n",
    "ext_test_data_path_infected = \"data_upload_v2\\\\test\\\\covid\"\n",
    "ext_test_data_path_normal = \"data_upload_v2\\\\test\\\\non\\\\normal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length before adding infected X-Rays to training data: 1700\n",
      "Length after adding infected X-Rays to training data: 1800\n",
      "Shape of training data: (1800, 224, 224, 3)\n",
      "Shape of training labels: (1800,)\n"
     ]
    }
   ],
   "source": [
    "Xtest_ext = []\n",
    "ytest_ext = []\n",
    "for imagePath in os.listdir(ext_test_data_path_normal):\n",
    "    img  = Image.open(ext_test_data_path_normal+'\\\\'+imagePath, 'r')\n",
    "    img = img.resize((224, 224))\n",
    "    img = img.convert('RGB')\n",
    "    Xtest_ext.append(np.array(img))\n",
    "    ytest_ext.append(ext_test_data_path_normal.split(os.path.sep)[-1])\n",
    "\n",
    "print(\"Length before adding infected X-Rays to training data:\", len(Xtest_ext))\n",
    "lenBefore = len(Xtest_ext)\n",
    "\n",
    "for imagePath in os.listdir(ext_test_data_path_infected):\n",
    "    img  = Image.open(ext_test_data_path_infected+'\\\\'+imagePath, 'r')\n",
    "    img = img.resize((224, 224))\n",
    "    img = img.convert('RGB')\n",
    "    Xtest_ext.append(np.array(img))\n",
    "    ytest_ext.append(ext_test_data_path_infected.split(os.path.sep)[-1])\n",
    "    \n",
    "print(\"Length after adding infected X-Rays to training data:\" ,len(Xtest_ext))\n",
    "Xtest_ext = np.array(Xtest_ext)\n",
    "ytest_ext = np.asarray(ytest_ext)\n",
    "print(\"Shape of training data:\", Xtest_ext.shape)\n",
    "print(\"Shape of training labels:\", ytest_ext.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('<U6')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytest_ext.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "ytest_ext = lb.fit_transform(ytest_ext)\n",
    "ytest_ext = to_categorical(ytest_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of testing data: (1800, 224, 224, 3)\n",
      "Shape of testing labels: (1800, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of testing data:\", Xtest_ext.shape)\n",
    "print(\"Shape of testing labels:\", ytest_ext.shape)\n",
    "\n",
    "Xtest_ext = Xtest_ext/255.0\n",
    "\n",
    "random.seed(4)\n",
    "random.shuffle(Xtest_ext)\n",
    "random.shuffle(ytest_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading first model (initialModel.h5)..\n",
      "Calculating accuracy against the testing data..\n",
      "1800/1800 [==============================] - 39s 22ms/step\n",
      "\n",
      "Loss: 7.584512244330512\n",
      "Accuracy: 0.04555555433034897\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "#First model \n",
    "print(\"Loading first model (initialModel.h5)..\")\n",
    "loadedModel = load_model(\"initial-Model.h5\")\n",
    "print(\"Calculating accuracy against the testing data..\")\n",
    "preds = loadedModel.evaluate(x = Xtest_ext, y = ytest_ext)\n",
    "print()\n",
    "print(\"Loss:\", preds[0])\n",
    "print(\"Accuracy:\", preds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Second model (vaibhavModel.h5)..\n",
      "WARNING:tensorflow:From c:\\users\\varun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From c:\\users\\varun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From c:\\users\\varun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From c:\\users\\varun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Calculating accuracy against the testing data..\n",
      "1800/1800 [==============================] - 141s 79ms/sample - loss: 12.3313 - acc: 0.0850\n",
      "\n",
      "Loss: 12.331262130737304\n",
      "Accuracy: 0.085\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "#second model\n",
    "print(\"Loading Second model (vaibhavModel.h5)..\")\n",
    "loadedModel = load_model(\"vaibhavModel.h5\")\n",
    "print(\"Calculating accuracy against the testing data..\")\n",
    "preds = loadedModel.evaluate(x = Xtest_ext, y = ytest_ext)\n",
    "print()\n",
    "print(\"Loss:\", preds[0])\n",
    "print(\"Accuracy:\", preds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Third model (vggPretrainedModel.h5)..\n",
      "Calculating accuracy against the testing data..\n",
      "1800/1800 [==============================] - 367s 204ms/sample - loss: 3.7555 - acc: 0.1578\n",
      "\n",
      "Loss: 3.7555342737833657\n",
      "Accuracy: 0.15777777\n"
     ]
    }
   ],
   "source": [
    "#from tensorflow.keras.models import load_model\n",
    "#Third model\n",
    "print(\"Loading Third model (vggPretrainedModel.h5)..\")\n",
    "loadedModel = load_model(\"vggPretrainedModel.h5\")\n",
    "print(\"Calculating accuracy against the testing data..\")\n",
    "preds = loadedModel.evaluate(x = Xtest_ext, y = ytest_ext)\n",
    "print()\n",
    "print(\"Loss:\", preds[0])\n",
    "print(\"Accuracy:\", preds[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using CV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n"
     ]
    }
   ],
   "source": [
    "DIR_NAME = \"data_upload_v2_other_way\\\\\"\n",
    "imagePaths=[]\n",
    "for dirname, _, filenames in os.walk(DIR_NAME):\n",
    "    for filename in filenames:\n",
    "      imagePaths.append(os.path.join(dirname, filename))\n",
    "print(len(imagePaths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "data = []\n",
    "labels = []\n",
    "# loop over the image paths\n",
    "for imagePath in imagePaths:\n",
    "    # extract the class label from the filename\n",
    "    label = imagePath.split(os.path.sep)[-2]\n",
    "    # load the image, swap color channels, and resize it to be a fixed\n",
    "    # 224x224 pixels while ignoring aspect ratio\n",
    "    image = cv2.imread(imagePath)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    # update the data and labels lists, respectively\n",
    "    data.append(image)\n",
    "    labels.append(label)\n",
    "# convert the data and labels to NumPy arrays while scaling the pixel\n",
    "# intensities to the range [0, 1]\n",
    "data = np.array(data) / 255.0\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "lb = LabelBinarizer()\n",
    "labelss = lb.fit_transform(labels)\n",
    "labelss = to_categorical(labelss, 2)\n",
    "print(labelss)\n",
    "# partition the data into training and testing splits using 80% of\n",
    "# the data for training and the remaining 20% for testing\n",
    "#(Xtrain, Xtest, ytrain, ytest) = train_test_split(data, labels,test_size=0.20, stratify=labels, random_state=42)\n",
    "# initialize the training data augmentation object\n",
    "trainAug = ImageDataGenerator(shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=15,\n",
    "    fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1800, 224, 224, 3), (1800, 2))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape,labelss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('normal', array([0., 1.], dtype=float32))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[-1], labelss[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 224, 224, 3)\n",
      "(1800, 2)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(4)\n",
    "random.shuffle(data)\n",
    "random.shuffle(labelss)\n",
    "print(data.shape)\n",
    "print(labelss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading first model (initialModel.h5)..\n",
      "Calculating accuracy against the testing data..\n",
      "1800/1800 [==============================] - 44s 24ms/step\n",
      "\n",
      "Loss: 6.227798445605569\n",
      "Accuracy: 0.22277778387069702\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "#First model \n",
    "print(\"Loading first model (initialModel.h5)..\")\n",
    "loadedModel = load_model(\"initial-Model.h5\")\n",
    "print(\"Calculating accuracy against the testing data..\")\n",
    "preds = loadedModel.evaluate(x = data, y = labelss)\n",
    "print()\n",
    "print(\"Loss:\", preds[0])\n",
    "print(\"Accuracy:\", preds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 224, 224, 3)\n",
      "Predicted: [[9.9990714e-01 9.2832022e-05]]\n",
      "Actual: [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "sampleData = data[-1].copy()\n",
    "sampleData = np.expand_dims(sampleData, axis=0)\n",
    "print(sampleData.shape)\n",
    "pred = loadedModel.predict(sampleData)\n",
    "print(\"Predicted:\",pred)\n",
    "print(\"Actual:\",labelss[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Second model (vaibhavModel.h5)..\n",
      "Calculating accuracy against the testing data..\n",
      "1800/1800 [==============================] - 376s 209ms/sample - loss: 10.2168 - acc: 0.2533\n",
      "\n",
      "Loss: 10.216842035923525\n",
      "Accuracy: 0.25333333\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "#second model\n",
    "print(\"Loading Second model (vaibhavModel.h5)..\")\n",
    "loadedModel = load_model(\"vaibhavModel.h5\")\n",
    "print(\"Calculating accuracy against the testing data..\")\n",
    "preds = loadedModel.evaluate(x = data, y = labelss)\n",
    "print()\n",
    "print(\"Loss:\", preds[0])\n",
    "print(\"Accuracy:\", preds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Third model (vggPretrainedModel.h5)..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-108-6e73e386962e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#Third model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loading Third model (vggPretrainedModel.h5)..\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mloadedModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"vggPretrainedModel.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Calculating accuracy against the testing data..\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloadedModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabelss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\varun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    144\u001b[0m       h5py is not None and (\n\u001b[0;32m    145\u001b[0m           isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\n\u001b[1;32m--> 146\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mhdf5_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model_from_hdf5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\varun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\hdf5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    254\u001b[0m           \u001b[0moptimizer_weight_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_optimizer_weights_from_hdf5_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m           \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer_weight_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m             logging.warning('Error in loading the saved optimizer '\n",
      "\u001b[1;32mc:\\users\\varun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\adam.py\u001b[0m in \u001b[0;36mset_weights\u001b[1;34m(self, weights)\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_vars\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m       \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_resource_apply_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\varun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mset_weights\u001b[1;34m(self, weights)\u001b[0m\n\u001b[0;32m    722\u001b[0m       \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m     \u001b[0mweight_value_tuples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 724\u001b[1;33m     \u001b[0mparam_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    725\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mpv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mpv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\varun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mbatch_get_value\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m   3008\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cannot get value inside Tensorflow graph function.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3009\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3010\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3011\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3012\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\varun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mget_session\u001b[1;34m(op_input_list)\u001b[0m\n\u001b[0;32m    460\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m       \u001b[0m_initialize_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\varun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[1;34m(session)\u001b[0m\n\u001b[0;32m    877\u001b[0m     \u001b[1;31m# marked as initialized.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    878\u001b[0m     is_initialized = session.run(\n\u001b[1;32m--> 879\u001b[1;33m         [variables_module.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[0;32m    880\u001b[0m     \u001b[0muninitialized_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\varun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\varun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1173\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\varun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1350\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\varun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1354\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1355\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1356\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1357\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\varun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1337\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1338\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1339\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m   1341\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "\u001b[1;32mc:\\users\\varun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1372\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1373\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1374\u001b[1;33m       \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1376\u001b[0m   \u001b[1;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#from tensorflow.keras.models import load_model\n",
    "#Third model\n",
    "print(\"Loading Third model (vggPretrainedModel.h5)..\")\n",
    "loadedModel = load_model(\"vggPretrainedModel.h5\")\n",
    "print(\"Calculating accuracy against the testing data..\")\n",
    "preds = loadedModel.evaluate(x = data, y = labelss)\n",
    "print()\n",
    "print(\"Loss:\", preds[0])\n",
    "print(\"Accuracy:\", preds[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing extra data to reduce overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3260\n"
     ]
    }
   ],
   "source": [
    "DIR_NAME = \"Combined-dataset\\\\\"\n",
    "imagePaths=[]\n",
    "for dirname, _, filenames in os.walk(DIR_NAME):\n",
    "    for filename in filenames:\n",
    "      imagePaths.append(os.path.join(dirname, filename))\n",
    "print(len(imagePaths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (3260, 224, 224, 3)\n",
      "labels shape: (3260,)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "data = []\n",
    "labels = []\n",
    "# loop over the image paths\n",
    "for imagePath in imagePaths:\n",
    "    # extract the class label from the filename\n",
    "    label = imagePath.split(os.path.sep)[-2]\n",
    "    # load the image, swap color channels, and resize it to be a fixed\n",
    "    # 224x224 pixels while ignoring aspect ratio\n",
    "    image = cv2.imread(imagePath)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    # update the data and labels lists, respectively\n",
    "    data.append(image)\n",
    "    labels.append(label)\n",
    "# convert the data and labels to NumPy arrays while scaling the pixel\n",
    "# intensities to the range [0, 1]\n",
    "data = np.array(data) / 255.0\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(\"Data shape:\",data.shape)\n",
    "print(\"labels shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "lb = LabelBinarizer()\n",
    "labelss = lb.fit_transform(labels)\n",
    "labelss = to_categorical(labelss)\n",
    "#print(labels)\n",
    "# partition the data into training and testing splits using 80% of\n",
    "# the data for training and the remaining 20% for testing\n",
    "(Xtrain, Xtest, ytrain, ytest) = train_test_split(data, labelss,test_size=0.30, stratify=labels, random_state=42)\n",
    "# initialize the training data augmentation object\n",
    "trainAug = ImageDataGenerator(shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=15,\n",
    "    fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Xtest, Xdev, ytest, ydev) = train_test_split(Xtest, ytest, test_size=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2282, 224, 224, 3),\n",
       " (489, 224, 224, 3),\n",
       " (489, 224, 224, 3),\n",
       " (2282, 2),\n",
       " (489, 2),\n",
       " (489, 2))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape, Xtest.shape, Xdev.shape, ytrain.shape, ytest.shape, ydev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\varun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "initialModel = initialSampleModel(Xtrain.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialModel.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\varun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From c:\\users\\varun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 2282 samples, validate on 489 samples\n",
      "Epoch 1/34\n",
      "2282/2282 [==============================] - 163s 71ms/step - loss: 0.2985 - accuracy: 0.8852 - val_loss: 1.1906 - val_accuracy: 0.6258\n",
      "Epoch 2/34\n",
      "2282/2282 [==============================] - 157s 69ms/step - loss: 0.1579 - accuracy: 0.9395 - val_loss: 2.4370 - val_accuracy: 0.4110\n",
      "Epoch 3/34\n",
      "2282/2282 [==============================] - 152s 67ms/step - loss: 0.1115 - accuracy: 0.9597 - val_loss: 0.6143 - val_accuracy: 0.7791\n",
      "Epoch 4/34\n",
      "2282/2282 [==============================] - 158s 69ms/step - loss: 0.1173 - accuracy: 0.9628 - val_loss: 1.0754 - val_accuracy: 0.7280\n",
      "Epoch 5/34\n",
      "2282/2282 [==============================] - 154s 68ms/step - loss: 0.0976 - accuracy: 0.9636 - val_loss: 0.5040 - val_accuracy: 0.7812\n",
      "Epoch 6/34\n",
      "2282/2282 [==============================] - 154s 67ms/step - loss: 0.0761 - accuracy: 0.9706 - val_loss: 0.7310 - val_accuracy: 0.8016\n",
      "Epoch 7/34\n",
      "2282/2282 [==============================] - 156s 68ms/step - loss: 0.0750 - accuracy: 0.9733 - val_loss: 1.2234 - val_accuracy: 0.7342\n",
      "Epoch 8/34\n",
      "2282/2282 [==============================] - 156s 68ms/step - loss: 0.0800 - accuracy: 0.9711 - val_loss: 1.1022 - val_accuracy: 0.6483\n",
      "Epoch 9/34\n",
      "2282/2282 [==============================] - 159s 70ms/step - loss: 0.0821 - accuracy: 0.9702 - val_loss: 0.3371 - val_accuracy: 0.8650\n",
      "Epoch 10/34\n",
      "2282/2282 [==============================] - 153s 67ms/step - loss: 0.0591 - accuracy: 0.9790 - val_loss: 0.0797 - val_accuracy: 0.9796\n",
      "Epoch 11/34\n",
      "2282/2282 [==============================] - 155s 68ms/step - loss: 0.0460 - accuracy: 0.9833 - val_loss: 0.1672 - val_accuracy: 0.9387\n",
      "Epoch 12/34\n",
      "2282/2282 [==============================] - 155s 68ms/step - loss: 0.0483 - accuracy: 0.9829 - val_loss: 0.2583 - val_accuracy: 0.9039\n",
      "Epoch 13/34\n",
      "2282/2282 [==============================] - 214s 94ms/step - loss: 0.0470 - accuracy: 0.9851 - val_loss: 0.2059 - val_accuracy: 0.9468\n",
      "Epoch 14/34\n",
      "2282/2282 [==============================] - 240s 105ms/step - loss: 0.0448 - accuracy: 0.9864 - val_loss: 0.1797 - val_accuracy: 0.9264\n",
      "Epoch 15/34\n",
      "2282/2282 [==============================] - 244s 107ms/step - loss: 0.0363 - accuracy: 0.9842 - val_loss: 0.4767 - val_accuracy: 0.8834\n",
      "Epoch 16/34\n",
      "2282/2282 [==============================] - 241s 106ms/step - loss: 0.0327 - accuracy: 0.9886 - val_loss: 0.2700 - val_accuracy: 0.9141\n",
      "Epoch 17/34\n",
      "2282/2282 [==============================] - 242s 106ms/step - loss: 0.0258 - accuracy: 0.9912 - val_loss: 0.2866 - val_accuracy: 0.9305\n",
      "Epoch 18/34\n",
      "2282/2282 [==============================] - 242s 106ms/step - loss: 0.0318 - accuracy: 0.9886 - val_loss: 0.3536 - val_accuracy: 0.8773\n",
      "Epoch 19/34\n",
      "2282/2282 [==============================] - 241s 106ms/step - loss: 0.0293 - accuracy: 0.9895 - val_loss: 0.1689 - val_accuracy: 0.9448\n",
      "Epoch 20/34\n",
      "2282/2282 [==============================] - 242s 106ms/step - loss: 0.0453 - accuracy: 0.9847 - val_loss: 1.4101 - val_accuracy: 0.7035\n",
      "Epoch 21/34\n",
      "2282/2282 [==============================] - 242s 106ms/step - loss: 0.0212 - accuracy: 0.9926 - val_loss: 0.1393 - val_accuracy: 0.9632\n",
      "Epoch 22/34\n",
      "2282/2282 [==============================] - 241s 106ms/step - loss: 0.0300 - accuracy: 0.9912 - val_loss: 0.1176 - val_accuracy: 0.9652\n",
      "Epoch 23/34\n",
      "2282/2282 [==============================] - 242s 106ms/step - loss: 0.0207 - accuracy: 0.9939 - val_loss: 0.1508 - val_accuracy: 0.9591\n",
      "Epoch 24/34\n",
      "2282/2282 [==============================] - 244s 107ms/step - loss: 0.0159 - accuracy: 0.9947 - val_loss: 0.1414 - val_accuracy: 0.9673\n",
      "Epoch 25/34\n",
      "2282/2282 [==============================] - 240s 105ms/step - loss: 0.0225 - accuracy: 0.9917 - val_loss: 0.3794 - val_accuracy: 0.8793\n",
      "Epoch 26/34\n",
      "2282/2282 [==============================] - 240s 105ms/step - loss: 0.0210 - accuracy: 0.9930 - val_loss: 0.7101 - val_accuracy: 0.7689\n",
      "Epoch 27/34\n",
      "2282/2282 [==============================] - 241s 105ms/step - loss: 0.0261 - accuracy: 0.9912 - val_loss: 0.1791 - val_accuracy: 0.9346\n",
      "Epoch 28/34\n",
      "2282/2282 [==============================] - 247s 108ms/step - loss: 0.0164 - accuracy: 0.9934 - val_loss: 0.1269 - val_accuracy: 0.9591\n",
      "Epoch 29/34\n",
      "2282/2282 [==============================] - 241s 106ms/step - loss: 0.0098 - accuracy: 0.9969 - val_loss: 0.0855 - val_accuracy: 0.9796\n",
      "Epoch 30/34\n",
      "2282/2282 [==============================] - 241s 106ms/step - loss: 0.0190 - accuracy: 0.9934 - val_loss: 0.1637 - val_accuracy: 0.9448\n",
      "Epoch 31/34\n",
      "2282/2282 [==============================] - 247s 108ms/step - loss: 0.0157 - accuracy: 0.9952 - val_loss: 2.2147 - val_accuracy: 0.7362\n",
      "Epoch 32/34\n",
      "2282/2282 [==============================] - 246s 108ms/step - loss: 0.0202 - accuracy: 0.9921 - val_loss: 0.5874 - val_accuracy: 0.8262\n",
      "Epoch 33/34\n",
      "2282/2282 [==============================] - 260s 114ms/step - loss: 0.0300 - accuracy: 0.9895 - val_loss: 0.3478 - val_accuracy: 0.9059\n",
      "Epoch 34/34\n",
      "2282/2282 [==============================] - 254s 111ms/step - loss: 0.0163 - accuracy: 0.9956 - val_loss: 0.0741 - val_accuracy: 0.9755\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1b899aa76c8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialModel.fit(x=Xtrain, y=ytrain, validation_data=(Xdev, ydev), epochs=34, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "489/489 [==============================] - 6s 12ms/step\n",
      "\n",
      "Loss: 0.04434444275959623\n",
      "Accuracy: 0.9815950989723206\n"
     ]
    }
   ],
   "source": [
    "preds = initialModel.evaluate(x = Xtest, y = ytest)\n",
    "print()\n",
    "print(\"Loss:\", preds[0])\n",
    "print(\"Accuracy:\", preds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialModel.save(\"initial-Model-extraData.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Xtrain.npy\", Xtrain)\n",
    "np.save(\"ytrain.npy\", ytrain)\n",
    "np.save(\"Xdev.npy\", Xdev)\n",
    "np.save(\"ydev.npy\", ydev)\n",
    "np.save(\"Xtest.npy\", Xtest)\n",
    "np.save(\"ytest.npy\", ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = np.load(\"Xtrain.npy\")\n",
    "Xtest = np.load(\"Xtest.npy\")\n",
    "Xdev = np.load(\"Xdev.npy\")\n",
    "ytrain = np.load(\"ytrain.npy\")\n",
    "ytest = np.load(\"ytest.npy\")\n",
    "ydev = np.load(\"ydev.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2282, 224, 224, 3),\n",
       " (489, 224, 224, 3),\n",
       " (489, 224, 224, 3),\n",
       " (2282, 2),\n",
       " (489, 2),\n",
       " (489, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape, Xtest.shape, Xdev.shape, ytrain.shape, ytest.shape, ydev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "[INFO] training head...\n",
      "Train on 2282 samples, validate on 489 samples\n",
      "WARNING:tensorflow:From c:\\users\\varun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/34\n",
      "2282/2282 [==============================] - 549s 241ms/sample - loss: 0.5069 - acc: 0.7642 - val_loss: 0.3333 - val_acc: 0.8855\n",
      "Epoch 2/34\n",
      "2282/2282 [==============================] - 545s 239ms/sample - loss: 0.3500 - acc: 0.8563 - val_loss: 0.2638 - val_acc: 0.9018\n",
      "Epoch 3/34\n",
      "2282/2282 [==============================] - 547s 240ms/sample - loss: 0.3037 - acc: 0.8791 - val_loss: 0.2330 - val_acc: 0.9182\n",
      "Epoch 4/34\n",
      "2282/2282 [==============================] - 545s 239ms/sample - loss: 0.2699 - acc: 0.8918 - val_loss: 0.2057 - val_acc: 0.9264\n",
      "Epoch 5/34\n",
      "2282/2282 [==============================] - 1101s 483ms/sample - loss: 0.2491 - acc: 0.8996 - val_loss: 0.2076 - val_acc: 0.9305\n",
      "Epoch 6/34\n",
      "2282/2282 [==============================] - 535s 234ms/sample - loss: 0.2335 - acc: 0.9115 - val_loss: 0.1874 - val_acc: 0.9346\n",
      "Epoch 7/34\n",
      "2282/2282 [==============================] - 427s 187ms/sample - loss: 0.2163 - acc: 0.9229 - val_loss: 0.1750 - val_acc: 0.9387\n",
      "Epoch 8/34\n",
      "2282/2282 [==============================] - 434s 190ms/sample - loss: 0.2060 - acc: 0.9176 - val_loss: 0.1669 - val_acc: 0.9366\n",
      "Epoch 9/34\n",
      "2282/2282 [==============================] - 422s 185ms/sample - loss: 0.2023 - acc: 0.9255 - val_loss: 0.1699 - val_acc: 0.9387\n",
      "Epoch 10/34\n",
      "2282/2282 [==============================] - 430s 188ms/sample - loss: 0.1829 - acc: 0.9338 - val_loss: 0.1708 - val_acc: 0.9366\n",
      "Epoch 11/34\n",
      "2282/2282 [==============================] - 422s 185ms/sample - loss: 0.1802 - acc: 0.9356 - val_loss: 0.1662 - val_acc: 0.9366\n",
      "Epoch 12/34\n",
      "2282/2282 [==============================] - 423s 185ms/sample - loss: 0.1765 - acc: 0.9382 - val_loss: 0.1548 - val_acc: 0.9387\n",
      "Epoch 13/34\n",
      "2282/2282 [==============================] - 420s 184ms/sample - loss: 0.1703 - acc: 0.9369 - val_loss: 0.1480 - val_acc: 0.9468\n",
      "Epoch 14/34\n",
      "2282/2282 [==============================] - 423s 185ms/sample - loss: 0.1605 - acc: 0.9422 - val_loss: 0.1806 - val_acc: 0.9387\n",
      "Epoch 15/34\n",
      "2282/2282 [==============================] - 421s 185ms/sample - loss: 0.1655 - acc: 0.9365 - val_loss: 0.1403 - val_acc: 0.9448\n",
      "Epoch 16/34\n",
      "2282/2282 [==============================] - 424s 186ms/sample - loss: 0.1520 - acc: 0.9404 - val_loss: 0.1296 - val_acc: 0.9489\n",
      "Epoch 17/34\n",
      "2282/2282 [==============================] - 419s 184ms/sample - loss: 0.1525 - acc: 0.9452 - val_loss: 0.1318 - val_acc: 0.9489\n",
      "Epoch 18/34\n",
      "2282/2282 [==============================] - 424s 186ms/sample - loss: 0.1393 - acc: 0.9518 - val_loss: 0.1403 - val_acc: 0.9530\n",
      "Epoch 19/34\n",
      "2282/2282 [==============================] - 444s 194ms/sample - loss: 0.1410 - acc: 0.9496 - val_loss: 0.1218 - val_acc: 0.9550\n",
      "Epoch 20/34\n",
      "2282/2282 [==============================] - 440s 193ms/sample - loss: 0.1341 - acc: 0.9483 - val_loss: 0.1236 - val_acc: 0.9530\n",
      "Epoch 21/34\n",
      "2282/2282 [==============================] - 425s 186ms/sample - loss: 0.1361 - acc: 0.9465 - val_loss: 0.1373 - val_acc: 0.9530\n",
      "Epoch 22/34\n",
      "2282/2282 [==============================] - 432s 189ms/sample - loss: 0.1319 - acc: 0.9584 - val_loss: 0.1204 - val_acc: 0.9550\n",
      "Epoch 23/34\n",
      "2282/2282 [==============================] - 426s 187ms/sample - loss: 0.1186 - acc: 0.9562 - val_loss: 0.1165 - val_acc: 0.9611\n",
      "Epoch 24/34\n",
      "2282/2282 [==============================] - 426s 186ms/sample - loss: 0.1189 - acc: 0.9540 - val_loss: 0.1270 - val_acc: 0.9448\n",
      "Epoch 25/34\n",
      "2282/2282 [==============================] - 424s 186ms/sample - loss: 0.1202 - acc: 0.9566 - val_loss: 0.1158 - val_acc: 0.9591\n",
      "Epoch 26/34\n",
      "2282/2282 [==============================] - 423s 185ms/sample - loss: 0.1109 - acc: 0.9575 - val_loss: 0.1131 - val_acc: 0.9550\n",
      "Epoch 27/34\n",
      "2282/2282 [==============================] - 421s 185ms/sample - loss: 0.1120 - acc: 0.9597 - val_loss: 0.1352 - val_acc: 0.9550\n",
      "Epoch 28/34\n",
      "2282/2282 [==============================] - 422s 185ms/sample - loss: 0.1164 - acc: 0.9571 - val_loss: 0.1097 - val_acc: 0.9611\n",
      "Epoch 29/34\n",
      "2282/2282 [==============================] - 425s 186ms/sample - loss: 0.1043 - acc: 0.9645 - val_loss: 0.1085 - val_acc: 0.9673\n",
      "Epoch 30/34\n",
      "2282/2282 [==============================] - 437s 191ms/sample - loss: 0.0992 - acc: 0.9623 - val_loss: 0.1238 - val_acc: 0.9571\n",
      "Epoch 31/34\n",
      "2282/2282 [==============================] - 428s 187ms/sample - loss: 0.1069 - acc: 0.9575 - val_loss: 0.1125 - val_acc: 0.9652\n",
      "Epoch 32/34\n",
      "2282/2282 [==============================] - 428s 188ms/sample - loss: 0.0953 - acc: 0.9636 - val_loss: 0.1081 - val_acc: 0.9652\n",
      "Epoch 33/34\n",
      "2282/2282 [==============================] - 427s 187ms/sample - loss: 0.0917 - acc: 0.9632 - val_loss: 0.1307 - val_acc: 0.9448\n",
      "Epoch 34/34\n",
      "2282/2282 [==============================] - 426s 186ms/sample - loss: 0.0938 - acc: 0.9632 - val_loss: 0.1078 - val_acc: 0.9693\n"
     ]
    }
   ],
   "source": [
    "baseModel = VGG16(weights=\"imagenet\", include_top=False,\n",
    "    input_tensor=Input(shape=(224, 224, 3)))\n",
    "# construct the head of the model that will be placed on top of the\n",
    "# the base model\n",
    "headModel = baseModel.output\n",
    "headModel = AveragePooling2D(pool_size=(4, 4))(headModel)\n",
    "headModel = Flatten(name=\"flatten\")(headModel)\n",
    "headModel = Dense(64, activation=\"relu\")(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
    "# place the head FC model on top of the base model (this will become\n",
    "# the actual model we will train)\n",
    "model = Model(inputs=baseModel.input, outputs=headModel)\n",
    "\n",
    "for layer in baseModel.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "INIT_LR = 1e-3\n",
    "EPOCHS = 34\n",
    "BS = 16\n",
    "# compile our model\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
    "\tmetrics=[\"accuracy\"])\n",
    "# train the head of the network\n",
    "print(\"[INFO] training head...\")\n",
    "H = model.fit(\n",
    "\tx=Xtrain, y=ytrain, batch_size=16,\n",
    "\tvalidation_data=(Xdev, ydev),\n",
    "\tepochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "489/489 [==============================] - 69s 142ms/sample - loss: 0.1311 - acc: 0.9550\n",
      "\n",
      "Loss: 0.13108124872831464\n",
      "Accuracy: 0.95501024\n"
     ]
    }
   ],
   "source": [
    "preds = model.evaluate(x = Xtest, y = ytest)\n",
    "print()\n",
    "print(\"Loss:\", preds[0])\n",
    "print(\"Accuracy:\", preds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"vggPretrainedModel-extraData.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "ImageInput (InputLayer)      [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "Conv1_1 (Conv2D)             (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "Conv1_2 (Conv2D)             (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling2D)         (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "Conv2_1 (SeparableConv2D)    (None, 112, 112, 128)     8896      \n",
      "_________________________________________________________________\n",
      "Conv2_2 (SeparableConv2D)    (None, 112, 112, 128)     17664     \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling2D)         (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "Conv3_1 (SeparableConv2D)    (None, 56, 56, 256)       34176     \n",
      "_________________________________________________________________\n",
      "bn1 (BatchNormalization)     (None, 56, 56, 256)       1024      \n",
      "_________________________________________________________________\n",
      "Conv3_2 (SeparableConv2D)    (None, 56, 56, 256)       68096     \n",
      "_________________________________________________________________\n",
      "bn2 (BatchNormalization)     (None, 56, 56, 256)       1024      \n",
      "_________________________________________________________________\n",
      "Conv3_3 (SeparableConv2D)    (None, 56, 56, 256)       68096     \n",
      "_________________________________________________________________\n",
      "pool3 (MaxPooling2D)         (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 200704)            0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 512)               102760960 \n",
      "_________________________________________________________________\n",
      "dropout1 (Dropout)           (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout2 (Dropout)           (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "fc3 (Dense)                  (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 103,064,578\n",
      "Trainable params: 103,063,554\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "Train on 2282 samples, validate on 489 samples\n",
      "WARNING:tensorflow:From c:\\users\\varun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/34\n",
      "2282/2282 [==============================] - 1102s 483ms/sample - loss: 0.5216 - acc: 0.7314 - val_loss: 0.8975 - val_acc: 0.3906\n",
      "Epoch 2/34\n",
      "2282/2282 [==============================] - 980s 429ms/sample - loss: 0.2849 - acc: 0.8896 - val_loss: 1.2884 - val_acc: 0.3906\n",
      "Epoch 3/34\n",
      "2282/2282 [==============================] - 1003s 440ms/sample - loss: 0.1924 - acc: 0.9251 - val_loss: 1.5057 - val_acc: 0.3906\n",
      "Epoch 4/34\n",
      "2282/2282 [==============================] - 1077s 472ms/sample - loss: 0.1549 - acc: 0.9461 - val_loss: 1.0655 - val_acc: 0.4008\n",
      "Epoch 5/34\n",
      "2282/2282 [==============================] - 1617s 709ms/sample - loss: 0.1095 - acc: 0.9636 - val_loss: 0.8239 - val_acc: 0.6585\n",
      "Epoch 6/34\n",
      "2282/2282 [==============================] - 971s 426ms/sample - loss: 0.0868 - acc: 0.9737 - val_loss: 0.2021 - val_acc: 0.9305\n",
      "Epoch 7/34\n",
      "2282/2282 [==============================] - 963s 422ms/sample - loss: 0.0762 - acc: 0.9715 - val_loss: 0.1109 - val_acc: 0.9652\n",
      "Epoch 8/34\n",
      "2282/2282 [==============================] - 835s 366ms/sample - loss: 0.0777 - acc: 0.9728 - val_loss: 0.3983 - val_acc: 0.8875\n",
      "Epoch 9/34\n",
      "2282/2282 [==============================] - 831s 364ms/sample - loss: 0.0631 - acc: 0.9781 - val_loss: 0.1302 - val_acc: 0.9571\n",
      "Epoch 10/34\n",
      "2282/2282 [==============================] - 825s 362ms/sample - loss: 0.0600 - acc: 0.9790 - val_loss: 0.0658 - val_acc: 0.9734\n",
      "Epoch 11/34\n",
      "2282/2282 [==============================] - 829s 363ms/sample - loss: 0.0465 - acc: 0.9847 - val_loss: 0.0547 - val_acc: 0.9796\n",
      "Epoch 12/34\n",
      "2282/2282 [==============================] - 825s 361ms/sample - loss: 0.0476 - acc: 0.9838 - val_loss: 0.1080 - val_acc: 0.9571\n",
      "Epoch 13/34\n",
      "2282/2282 [==============================] - 831s 364ms/sample - loss: 0.0348 - acc: 0.9899 - val_loss: 0.0535 - val_acc: 0.9796\n",
      "Epoch 14/34\n",
      "2282/2282 [==============================] - 825s 361ms/sample - loss: 0.0273 - acc: 0.9890 - val_loss: 0.5320 - val_acc: 0.9018\n",
      "Epoch 15/34\n",
      "2282/2282 [==============================] - 828s 363ms/sample - loss: 0.0403 - acc: 0.9877 - val_loss: 0.1319 - val_acc: 0.9632\n",
      "Epoch 16/34\n",
      "1120/2282 [=============>................] - ETA: 7:47 - loss: 0.0369 - acc: 0.9884"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-8ffcb110113f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#chkpt = callbacks.ModelCheckpoint(filepath='best_model_todate', save_best_only=True, save_weights_only=False)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXdev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mydev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\varun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\varun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\varun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32mc:\\users\\varun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "INIT_LR = 1e-3\n",
    "EPOCHS = 34\n",
    "BS = 16\n",
    "model =  vaibhav_model()\n",
    "model.summary()\n",
    "opt = Adam(lr=0.0001, decay=1e-5)\n",
    "#es = callbacks.EarlyStopping(patience=18)\n",
    "#chkpt = callbacks.ModelCheckpoint(filepath='best_model_todate', save_best_only=True, save_weights_only=False)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'],optimizer=opt)\n",
    "history = model.fit(x = Xtrain, y = ytrain, batch_size=16, epochs=EPOCHS, validation_data=(Xdev, ydev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\varun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "ImageInput (InputLayer)      [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "Conv1_1 (Conv2D)             (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "Conv1_2 (Conv2D)             (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling2D)         (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "Conv2_1 (SeparableConv2D)    (None, 112, 112, 128)     8896      \n",
      "_________________________________________________________________\n",
      "Conv2_2 (SeparableConv2D)    (None, 112, 112, 128)     17664     \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling2D)         (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "Conv3_1 (SeparableConv2D)    (None, 56, 56, 256)       34176     \n",
      "_________________________________________________________________\n",
      "bn1 (BatchNormalization)     (None, 56, 56, 256)       1024      \n",
      "_________________________________________________________________\n",
      "Conv3_2 (SeparableConv2D)    (None, 56, 56, 256)       68096     \n",
      "_________________________________________________________________\n",
      "bn2 (BatchNormalization)     (None, 56, 56, 256)       1024      \n",
      "_________________________________________________________________\n",
      "Conv3_3 (SeparableConv2D)    (None, 56, 56, 256)       68096     \n",
      "_________________________________________________________________\n",
      "pool3 (MaxPooling2D)         (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 200704)            0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 512)               102760960 \n",
      "_________________________________________________________________\n",
      "dropout1 (Dropout)           (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout2 (Dropout)           (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "fc3 (Dense)                  (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 103,064,578\n",
      "Trainable params: 103,063,554\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_vaibhav_model():\n",
    "    input_img = Input(shape=(224,224,3), name='ImageInput')\n",
    "    x = Conv2D(64, (3,3), activation='relu', padding='same', name='Conv1_1')(input_img)\n",
    "    x = Conv2D(64, (3,3), activation='relu', padding='same', name='Conv1_2')(x)\n",
    "    x = MaxPooling2D((2,2), name='pool1')(x)\n",
    "    \n",
    "    x = SeparableConv2D(128, (3,3), activation='relu', padding='same', name='Conv2_1')(x)\n",
    "    x = SeparableConv2D(128, (3,3), activation='relu', padding='same', name='Conv2_2')(x)\n",
    "    x = MaxPooling2D((2,2), name='pool2')(x)\n",
    "    \n",
    "    x = SeparableConv2D(256, (3,3), activation='relu', padding='same', name='Conv3_1')(x)\n",
    "    x = BatchNormalization(name='bn1')(x)\n",
    "    x = SeparableConv2D(256, (3,3), activation='relu', padding='same', name='Conv3_2')(x)\n",
    "    x = BatchNormalization(name='bn2')(x)\n",
    "    x = SeparableConv2D(256, (3,3), activation='relu', padding='same', name='Conv3_3')(x)\n",
    "    x = MaxPooling2D((2,2), name='pool3')(x)\n",
    "    x = Flatten(name='flatten')(x)\n",
    "    x = Dense(512, activation='relu', name='fc1')(x)\n",
    "    x = Dropout(0.5, name='dropout1')(x)\n",
    "    x = Dense(128, activation='relu', name='fc2')(x)\n",
    "    x = Dropout(0.5, name='dropout2')(x)\n",
    "    x = Dense(2, activation='softmax', name='fc3')(x)\n",
    "    \n",
    "    model = Model(inputs=input_img, outputs=x)\n",
    "    return model\n",
    "loadedModel =  build_vaibhav_model()\n",
    "loadedModel.summary()\n",
    "opt = Adam(lr=0.0001, decay=1e-5)\n",
    "es = callbacks.EarlyStopping(patience=18)\n",
    "chkpt = callbacks.ModelCheckpoint(filepath='best_model_todate', save_best_only=True, save_weights_only=False)\n",
    "loadedModel.compile(loss='binary_crossentropy', metrics=['accuracy'],optimizer=opt)\n",
    "loadedModel.load_weights(\"vaibhav-Model-extraData.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Second model (vaibhavModel.h5)..\n",
      "Calculating accuracy against the testing data..\n",
      "489/489 [==============================] - 43s 87ms/sample - loss: 0.0568 - acc: 0.9918\n",
      "\n",
      "Loss: 0.056801250857356045\n",
      "Accuracy: 0.99182004\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "#second model\n",
    "print(\"Loading Second model (vaibhavModel.h5)..\")\n",
    "#loadedModel = load_model(\"vaibhav-Model-extraData-1.h5\")\n",
    "print(\"Calculating accuracy against the testing data..\")\n",
    "preds = loadedModel.evaluate(x = Xtest, y = ytest)\n",
    "print()\n",
    "print(\"Loss:\", preds[0])\n",
    "print(\"Accuracy:\", preds[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadedModel.save(\"vaibhavModel-Final.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\varun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From c:\\users\\varun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From c:\\users\\varun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Loading Second model (vaibhavModel.h5)..\n",
      "Calculating accuracy against the testing data..\n",
      "489/489 [==============================] - 37s 77ms/sample - loss: 0.0568 - acc: 0.9918\n",
      "\n",
      "Loss: 0.056801250857356045\n",
      "Accuracy: 0.99182004\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "LM = load_model(\"vaibhavModel-Final.h5\")\n",
    "print(\"Loading Second model (vaibhavModel.h5)..\")\n",
    "#loadedModel = load_model(\"vaibhav-Model-extraData-1.h5\")\n",
    "print(\"Calculating accuracy against the testing data..\")\n",
    "preds = LM.evaluate(x = Xtest, y = ytest)\n",
    "print()\n",
    "print(\"Loss:\", preds[0])\n",
    "print(\"Accuracy:\", preds[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
